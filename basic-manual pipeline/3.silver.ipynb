{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "329dd2d5-3bb0-48e5-bfec-66796bcd0ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Null value check for table [bronze_daily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c6f6c6-80f1-40b8-bbc0-0e15f2a32dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df = spark.read.table(\"kenworkspace.tw_stocks_db.bronze_daily\")\n",
    "\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63a934ac-2cde-4110-99dc-e1ccba241562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Null value check for table [bronze_monthly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c1aac4-0027-4a0b-809b-8ecb4b6ec945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"kenworkspace.tw_stocks_db.bronze_monthly\")\n",
    "\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23ec55fa-af4b-475b-b9a5-7fadd12e5a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature engineering for table [bronze_monthly] --> [silver_mvrh_monthly]\n",
    "### 1. New column: monthly volume record high [MVRH]\n",
    "### 2. New column: Distance (%) from last MVRH [from_MVRH_percent]\n",
    "### 3. Get rid of the symbol column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d6e5f4f-9db6-4b52-ba79-080ac6f04e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE kenworkspace.tw_stocks_db.silver_mvrh_monthly AS\n",
    "  WITH base AS (\n",
    "    SELECT\n",
    "      `date`,\n",
    "      `open`,\n",
    "      `high`,\n",
    "      `low`,\n",
    "      `close`,\n",
    "      `volume`,\n",
    "      MAX(`volume`) OVER (ORDER BY `date`) AS max_volume_so_far\n",
    "    FROM kenworkspace.tw_stocks_db.bronze_monthly\n",
    "  ),\n",
    "  mvrh_flagged AS (\n",
    "    SELECT *,\n",
    "      volume = max_volume_so_far AS mvrh\n",
    "    FROM base\n",
    "  ),\n",
    "  add_previous_mvrh AS (\n",
    "    SELECT *,\n",
    "      -- find the last MVRH's volume（excluding current row）\n",
    "      LAG(\n",
    "        CASE WHEN mvrh THEN volume ELSE NULL END\n",
    "      ) IGNORE NULLS OVER (ORDER BY `date`) AS last_mvrh_volume\n",
    "    FROM mvrh_flagged\n",
    "  ),\n",
    "  final AS (\n",
    "    SELECT *,\n",
    "      -- distance from the last MVRH (%)\n",
    "      CASE \n",
    "        WHEN last_mvrh_volume IS NOT NULL THEN \n",
    "          ROUND((volume - last_mvrh_volume) / last_mvrh_volume * 100, 2)\n",
    "        ELSE NULL \n",
    "      END AS from_mvrh_percent\n",
    "    FROM add_previous_mvrh\n",
    "  )\n",
    "  SELECT * FROM final;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ac59bc-ff78-4e03-84f1-b00b56afed5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Feature engineering for table [bronze_daily] --> [silver_daily]\n",
    "###1. Get rid of the symbol column \n",
    "###2. New column: 60 days future highest return [60fhr_percent]\n",
    "###3. New column: 60 days future lowest return [60flr_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9847830-3f1c-45a2-b2f1-d14318fd23fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# read data\n",
    "df = spark.table(\"kenworkspace.tw_stocks_db.bronze_daily\").orderBy(\"date\")\n",
    "\n",
    "#  pandas_udf: pd.Series as inputs, DataFrame as output\n",
    "@pandas_udf(returnType=StructType([\n",
    "    StructField(\"60fhr_percent\", DoubleType(), True),\n",
    "    StructField(\"60flr_percent\", DoubleType(), True)\n",
    "]))\n",
    "def calculate_future_returns(closes: pd.Series, highs: pd.Series, lows: pd.Series) -> pd.DataFrame:\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(closes)):\n",
    "        start_idx = i + 1\n",
    "        end_idx = min(i + 61, len(closes))\n",
    "        \n",
    "        # check if there are 60 days afterwards\n",
    "        if (end_idx - start_idx) < 60:\n",
    "            # not enough 60 days, return None\n",
    "            results.append({\"60fhr_percent\": None, \"60flr_percent\": None})\n",
    "            continue\n",
    "        \n",
    "        future_highs = highs.iloc[start_idx:end_idx]\n",
    "        future_lows = lows.iloc[start_idx:end_idx]\n",
    "        current_close = closes.iloc[i]\n",
    "\n",
    "        max_high = future_highs.max()\n",
    "        max_high_pos = future_highs.idxmax()  # 全局 index\n",
    "        max_high_relative_pos = max_high_pos - start_idx\n",
    "\n",
    "        fhr_percent = round((max_high - current_close) / current_close * 100, 2)\n",
    "\n",
    "        before_max_lows = future_lows.iloc[:max_high_relative_pos + 1]\n",
    "        min_low = before_max_lows.min()\n",
    "        flr_percent = round((min_low - current_close) / current_close * 100, 2)\n",
    "\n",
    "        results.append({\n",
    "            \"60fhr_percent\": fhr_percent,\n",
    "            \"60flr_percent\": flr_percent\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# select, pandas_udf, alias (pyspark)\n",
    "df_with_return = df.select(\n",
    "    \"*\",  # 保留所有欄位\n",
    "    calculate_future_returns(\n",
    "        col(\"close\"),\n",
    "        col(\"high\"),\n",
    "        col(\"low\")\n",
    "    ).alias(\"future_returns\")\n",
    ")\n",
    "\n",
    "#  fetch data in the struct \n",
    "result_df = df_with_return.select(\n",
    "    \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    col(\"future_returns.60fhr_percent\").alias(\"60fhr_percent\"),\n",
    "    col(\"future_returns.60flr_percent\").alias(\"60flr_percent\")\n",
    ")\n",
    "\n",
    "# write into table\n",
    "result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kenworkspace.tw_stocks_db.silver_daily\")\n",
    "\n",
    "print(\"Successfully created silver_daily table!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f194765a-e3c8-41c6-9067-673da5a39485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG kenworkspace;\n",
    "\n",
    "SELECT *\n",
    "FROM tw_stocks_db.silver_daily\n",
    "ORDER BY date DESC\n",
    "LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea42fd98-2c4c-42c9-83e0-d73798e2d984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5240455481320153,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3.silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
