{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c353ef-2e10-4d84-bd6b-777f5497a6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# bronze\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dlta_bronze_daily\",\n",
    "    comment=\"Ingest daily parquet files for 2330 from ADLS Gen2 using Autoloader\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"non_null_fields\", \"date IS NOT NULL AND open IS NOT NULL AND high IS NOT NULL AND low IS NOT NULL AND close IS NOT NULL AND volume IS NOT NULL AND symbol IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"positive_prices_and_volume\", \"open > 0 AND high > 0 AND low > 0 AND close > 0 AND volume > 0\")\n",
    "def dlt_bronze_daily():\n",
    "    df = (spark.readStream.format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"parquet\")\n",
    "            .load(\"abfss://twstocks@kenspractice.dfs.core.windows.net/daily/\"))\n",
    "    return df\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dlta_bronze_monthly\",\n",
    "    comment=\"Ingest monthly parquet files for 2330 from ADLS Gen2 using Autoloader\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"non_null_fields\", \"date IS NOT NULL AND open IS NOT NULL AND high IS NOT NULL AND low IS NOT NULL AND close IS NOT NULL AND volume IS NOT NULL AND symbol IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"positive_prices_and_volume\", \"open > 0 AND high > 0 AND low > 0 AND close > 0 AND volume > 0\")\n",
    "def dlt_bronze_monthly():\n",
    "    df = (spark.readStream.format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"parquet\")\n",
    "            .load(\"abfss://twstocks@kenspractice.dfs.core.windows.net/monthly/\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9847830-3f1c-45a2-b2f1-d14318fd23fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, round as spark_round, row_number,\n",
    "    max as spark_max, min as spark_min, count\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dlta_silver_daily\",\n",
    "    comment=\"準確版本：計算到最高價前的最低價，未來不滿60天返回null\"\n",
    ")\n",
    "def dlt_silver_daily():\n",
    "    \"\"\"\n",
    "    實現完全準確的邏輯：\n",
    "    - FHR: 未來60天內的最高價報酬率\n",
    "    - FLR: 從今天到達成最高價期間的最低價報酬率\n",
    "    - 未來不滿60天的記錄返回 null\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.read.table(\"kenworkspace.tw_stocks_db.dlta_bronze_daily\").orderBy(\"date\")\n",
    "    \n",
    "    # 添加行號\n",
    "    df = df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"date\")))\n",
    "    \n",
    "    # 建立未來資料的別名\n",
    "    future_df = df.select(\n",
    "        col(\"row_id\").alias(\"future_row_id\"),\n",
    "        col(\"date\").alias(\"future_date\"),\n",
    "        col(\"high\").alias(\"future_high\"),\n",
    "        col(\"low\").alias(\"future_low\")\n",
    "    )\n",
    "    \n",
    "    # 聯結條件：聯結未來1-60天的資料\n",
    "    joined_df = df.alias(\"current\").join(\n",
    "        future_df.alias(\"future\"),\n",
    "        (col(\"future.future_row_id\") > col(\"current.row_id\")) &\n",
    "        (col(\"future.future_row_id\") <= col(\"current.row_id\") + 60),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # 計算每個起始日期有多少天的未來資料\n",
    "    window_count = Window.partitionBy(\"current.row_id\")\n",
    "    joined_df = joined_df.withColumn(\n",
    "        \"future_days_count\", count(\"future.future_row_id\").over(window_count)\n",
    "    )\n",
    "    \n",
    "    # 計算每個起始日期的最高價（只處理有60天資料的）\n",
    "    window_max_high = Window.partitionBy(\"current.row_id\")\n",
    "    df_with_max = joined_df.withColumn(\n",
    "        \"period_max_high\", spark_max(\"future.future_high\").over(window_max_high)\n",
    "    )\n",
    "    \n",
    "    # 找出達到最高價的最早日期（只保留有60天資料的）\n",
    "    df_max_date = df_with_max.filter(\n",
    "        (col(\"future.future_high\") == col(\"period_max_high\")) &\n",
    "        (col(\"future_days_count\") >= 60)\n",
    "    ).withColumn(\n",
    "        \"max_date_rank\", \n",
    "        row_number().over(\n",
    "            Window.partitionBy(\"current.row_id\").orderBy(\"future.future_date\")\n",
    "        )\n",
    "    ).filter(col(\"max_date_rank\") == 1)\n",
    "    \n",
    "    # 對於每個起始日期，計算到最高價日期為止的最低價\n",
    "    min_period_df = df_max_date.alias(\"main\").join(\n",
    "        future_df.alias(\"min_data\"),\n",
    "        (col(\"min_data.future_row_id\") > col(\"main.row_id\")) &\n",
    "        (col(\"min_data.future_row_id\") <= col(\"main.future_row_id\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # 計算這期間的最低價\n",
    "    window_min_low = Window.partitionBy(\"main.row_id\")\n",
    "    df_with_min = min_period_df.withColumn(\n",
    "        \"period_min_low\", spark_min(\"min_data.future_low\").over(window_min_low)\n",
    "    )\n",
    "    \n",
    "    # 取得有60天資料的最終結果\n",
    "    processed_df = df_with_min.select(\n",
    "        col(\"main.row_id\"),\n",
    "        col(\"main.date\"),\n",
    "        col(\"main.open\"),\n",
    "        col(\"main.high\"), \n",
    "        col(\"main.low\"),\n",
    "        col(\"main.close\"),\n",
    "        col(\"main.volume\"),\n",
    "        col(\"main.period_max_high\"),\n",
    "        col(\"period_min_low\"),\n",
    "        col(\"main.future_days_count\")\n",
    "    ).distinct()\n",
    "    \n",
    "    # 將處理結果與原始資料聯結，確保所有日期都包含在內\n",
    "    all_dates_df = df.select(\"row_id\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\")\n",
    "    \n",
    "    final_df = all_dates_df.alias(\"all\").join(\n",
    "        processed_df.alias(\"processed\"),\n",
    "        col(\"all.row_id\") == col(\"processed.row_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # 計算百分比 - 只有未來滿60天的才計算，否則返回 null\n",
    "    result_df = final_df.withColumn(\n",
    "        \"60fhr_percent\",\n",
    "        when(\n",
    "            (col(\"processed.future_days_count\") >= 60) & col(\"processed.period_max_high\").isNotNull(),\n",
    "            spark_round(\n",
    "                ((col(\"processed.period_max_high\") - col(\"all.close\")) / col(\"all.close\")) * 100, 2\n",
    "            )\n",
    "        ).otherwise(None).cast(DoubleType())\n",
    "    ).withColumn(\n",
    "        \"60flr_percent\",\n",
    "        when(\n",
    "            (col(\"processed.future_days_count\") >= 60) & col(\"processed.period_min_low\").isNotNull(),\n",
    "            spark_round(\n",
    "                ((col(\"processed.period_min_low\") - col(\"all.close\")) / col(\"all.close\")) * 100, 2\n",
    "            )\n",
    "        ).otherwise(None).cast(DoubleType())\n",
    "    )\n",
    "    \n",
    "    return result_df.select(\n",
    "        col(\"all.date\"),\n",
    "        col(\"all.open\"),\n",
    "        col(\"all.high\"),\n",
    "        col(\"all.low\"),\n",
    "        col(\"all.close\"),\n",
    "        col(\"all.volume\"),\n",
    "        \"60fhr_percent\",\n",
    "        \"60flr_percent\"\n",
    "    ).orderBy(\"date\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7959278122817644,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2python.dlt_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
