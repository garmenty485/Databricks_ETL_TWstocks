{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c353ef-2e10-4d84-bd6b-777f5497a6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# bronze\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dlta_bronze_daily\",\n",
    "    comment=\"Ingest daily parquet files for 2330 from ADLS Gen2 using Autoloader\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"non_null_fields\", \"date IS NOT NULL AND open IS NOT NULL AND high IS NOT NULL AND low IS NOT NULL AND close IS NOT NULL AND volume IS NOT NULL AND symbol IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"positive_prices_and_volume\", \"open > 0 AND high > 0 AND low > 0 AND close > 0 AND volume > 0\")\n",
    "def dlt_bronze_daily():\n",
    "    df = (spark.readStream.format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"parquet\")\n",
    "            .load(\"abfss://twstocks@kenspractice.dfs.core.windows.net/daily/\"))\n",
    "    return df\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dlta_bronze_monthly\",\n",
    "    comment=\"Ingest monthly parquet files for 2330 from ADLS Gen2 using Autoloader\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"non_null_fields\", \"date IS NOT NULL AND open IS NOT NULL AND high IS NOT NULL AND low IS NOT NULL AND close IS NOT NULL AND volume IS NOT NULL AND symbol IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"positive_prices_and_volume\", \"open > 0 AND high > 0 AND low > 0 AND close > 0 AND volume > 0\")\n",
    "def dlt_bronze_monthly():\n",
    "    df = (spark.readStream.format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"parquet\")\n",
    "            .load(\"abfss://twstocks@kenspractice.dfs.core.windows.net/monthly/\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9847830-3f1c-45a2-b2f1-d14318fd23fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, round as spark_round, row_number,\n",
    "    max as spark_max, min as spark_min, count\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dlta_silver_daily\",\n",
    "    comment=\"Accurate version: Calculate the lowest price before the highest price within the next 60 days; return null if less than 60 days in the future\"\n",
    ")\n",
    "def dlt_silver_daily():\n",
    "    \"\"\"\n",
    "    Implements fully accurate logic:\n",
    "    - FHR: Maximum return within the next 60 days\n",
    "    - FLR: Minimum return from today until the date of the maximum price\n",
    "    - Records with less than 60 days in the future return null\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.read.table(\"kenworkspace.tw_stocks_db.dlta_bronze_daily\").orderBy(\"date\")\n",
    "    \n",
    "    # Add row number\n",
    "    df = df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"date\")))\n",
    "    \n",
    "    # Create alias for future data\n",
    "    future_df = df.select(\n",
    "        col(\"row_id\").alias(\"future_row_id\"),\n",
    "        col(\"date\").alias(\"future_date\"),\n",
    "        col(\"high\").alias(\"future_high\"),\n",
    "        col(\"low\").alias(\"future_low\")\n",
    "    )\n",
    "    \n",
    "    # Join condition: join future data from 1 to 60 days ahead\n",
    "    joined_df = df.alias(\"current\").join(\n",
    "        future_df.alias(\"future\"),\n",
    "        (col(\"future.future_row_id\") > col(\"current.row_id\")) &\n",
    "        (col(\"future.future_row_id\") <= col(\"current.row_id\") + 60),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Count how many future days are available for each start date\n",
    "    window_count = Window.partitionBy(\"current.row_id\")\n",
    "    joined_df = joined_df.withColumn(\n",
    "        \"future_days_count\", count(\"future.future_row_id\").over(window_count)\n",
    "    )\n",
    "    \n",
    "    # Calculate the maximum high price for each start date (only process those with 60 days of data)\n",
    "    window_max_high = Window.partitionBy(\"current.row_id\")\n",
    "    df_with_max = joined_df.withColumn(\n",
    "        \"period_max_high\", spark_max(\"future.future_high\").over(window_max_high)\n",
    "    )\n",
    "    \n",
    "    # Find the earliest date that reaches the maximum high (only keep those with 60 days of data)\n",
    "    df_max_date = df_with_max.filter(\n",
    "        (col(\"future.future_high\") == col(\"period_max_high\")) &\n",
    "        (col(\"future_days_count\") >= 60)\n",
    "    ).withColumn(\n",
    "        \"max_date_rank\", \n",
    "        row_number().over(\n",
    "            Window.partitionBy(\"current.row_id\").orderBy(\"future.future_date\")\n",
    "        )\n",
    "    ).filter(col(\"max_date_rank\") == 1)\n",
    "    \n",
    "    # For each start date, calculate the minimum low up to the date of the maximum high\n",
    "    min_period_df = df_max_date.alias(\"main\").join(\n",
    "        future_df.alias(\"min_data\"),\n",
    "        (col(\"min_data.future_row_id\") > col(\"main.row_id\")) &\n",
    "        (col(\"min_data.future_row_id\") <= col(\"main.future_row_id\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate the minimum low in this period\n",
    "    window_min_low = Window.partitionBy(\"main.row_id\")\n",
    "    df_with_min = min_period_df.withColumn(\n",
    "        \"period_min_low\", spark_min(\"min_data.future_low\").over(window_min_low)\n",
    "    )\n",
    "    \n",
    "    # Get the final result for those with 60 days of data\n",
    "    processed_df = df_with_min.select(\n",
    "        col(\"main.row_id\"),\n",
    "        col(\"main.date\"),\n",
    "        col(\"main.open\"),\n",
    "        col(\"main.high\"), \n",
    "        col(\"main.low\"),\n",
    "        col(\"main.close\"),\n",
    "        col(\"main.volume\"),\n",
    "        col(\"main.period_max_high\"),\n",
    "        col(\"period_min_low\"),\n",
    "        col(\"main.future_days_count\")\n",
    "    ).distinct()\n",
    "    \n",
    "    # Join the processed result with the original data to ensure all dates are included\n",
    "    all_dates_df = df.select(\"row_id\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\")\n",
    "    \n",
    "    final_df = all_dates_df.alias(\"all\").join(\n",
    "        processed_df.alias(\"processed\"),\n",
    "        col(\"all.row_id\") == col(\"processed.row_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate percentage - only compute if there are 60 future days, otherwise return null\n",
    "    result_df = final_df.withColumn(\n",
    "        \"60fhr_percent\",\n",
    "        when(\n",
    "            (col(\"processed.future_days_count\") >= 60) & col(\"processed.period_max_high\").isNotNull(),\n",
    "            spark_round(\n",
    "                ((col(\"processed.period_max_high\") - col(\"all.close\")) / col(\"all.close\")) * 100, 2\n",
    "            )\n",
    "        ).otherwise(None).cast(DoubleType())\n",
    "    ).withColumn(\n",
    "        \"60flr_percent\",\n",
    "        when(\n",
    "            (col(\"processed.future_days_count\") >= 60) & col(\"processed.period_min_low\").isNotNull(),\n",
    "            spark_round(\n",
    "                ((col(\"processed.period_min_low\") - col(\"all.close\")) / col(\"all.close\")) * 100, 2\n",
    "            )\n",
    "        ).otherwise(None).cast(DoubleType())\n",
    "    )\n",
    "    \n",
    "    return result_df.select(\n",
    "        col(\"all.date\"),\n",
    "        col(\"all.open\"),\n",
    "        col(\"all.high\"),\n",
    "        col(\"all.low\"),\n",
    "        col(\"all.close\"),\n",
    "        col(\"all.volume\"),\n",
    "        \"60fhr_percent\",\n",
    "        \"60flr_percent\"\n",
    "    ).orderBy(\"date\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7959278122817644,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2python.dlt_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
